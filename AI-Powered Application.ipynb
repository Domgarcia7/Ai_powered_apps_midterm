{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e06beea",
   "metadata": {},
   "source": [
    "# Personalized Music Recommender\n",
    "* Created by: Dominique Garcia\n",
    "* For: AI-Powered Applications\n",
    "* At: University of Texas Rio Grander Valley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0b1d4",
   "metadata": {},
   "source": [
    "### OpenAI API\n",
    "https://ericmichael-openai-playground-utrgv.hf.space/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a0bd40",
   "metadata": {},
   "source": [
    "#### Setup your .env file\n",
    "Setup your OPENAI_API_BASE key and OPENAI_API_KEY in a file .env in this same folder.\n",
    "\n",
    "#### example .env contents (copy paste this into a .env file)\n",
    "OPENAI_API_BASE=yourapibase\n",
    "OPENAI_API_KEY=yourapikey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b13b3",
   "metadata": {},
   "source": [
    "#### Install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54461976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d53d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Create the Dataset for top 50 songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd1fa9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_trending_songs.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_trending_songs.json\n",
    "[\n",
    "    {\n",
    "        \"Artist name\": \"Jack Harlow\",\n",
    "        \"Genre\": \"Hip-Hop\",\n",
    "        \"Song\": \"Lovin On Me\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Taylor Swift\",\n",
    "        \"Genre\": \"Pop\",\n",
    "        \"Song\": \"Is it Over now? (Taylor's Versio)(From the vault)\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Mitski\",\n",
    "        \"Genre\": \"Indie\",\n",
    "        \"Song\": \"My love Mine All Mine\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Zach Bryan, Kacey Musgraves\",\n",
    "        \"Genre\": \"Country\",\n",
    "        \"Song\": \"I remember Everything (feat. Kacey Musgraves)\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Tate McRae\",\n",
    "        \"Genre\": \"Pop\",\n",
    "        \"Song\": \"greedy\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Drake, Yeat\",\n",
    "        \"Genre\": \"Rap\",\n",
    "        \"Song\": \"IDGAF (feat. Yeat)\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Brenda Lee\",\n",
    "        \"Genre\": \"Holiday\",\n",
    "        \"Song\": \"Rockin' Around The Christmas Tree\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Doja Cat\",\n",
    "        \"Genre\": \"Hip-Hop\",\n",
    "        \"Song\": \"Agora Hills\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Noah Kahan\",\n",
    "        \"Genre\": \"Folk\",\n",
    "        \"Song\": \"Stick Season\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Drake, J. Cole\",\n",
    "        \"Genre\": \"Rap\",\n",
    "        \"Song\": \"First Person Shooter (feat. J. Cole)\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Taylor Swift\",\n",
    "        \"Genre\": \"Pop\",\n",
    "        \"Song\": \"Cruel Summer\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Mariah Carey\",\n",
    "        \"Genre\": \"Holiday\",\n",
    "        \"Song\": \"All I want for Christmas Is You\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Doja Cat\",\n",
    "        \"Genre\": \"Hip-Hop\",\n",
    "        \"Song\": \"Paint The Town Red\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Lil Tecca\",\n",
    "        \"Genre\": \"Rap\",\n",
    "        \"Song\": \"500lbs\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Drake\",\n",
    "        \"Genre\": \"Rap\",\n",
    "        \"Song\": \"You Broke My Heart\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Bing Crosby, Ken Darby Singers, John Scott Trotter & His Orchestra\",\n",
    "        \"Genre\": \"Holiday\",\n",
    "        \"Song\": \"White Christmas - 1947 Version\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Taylor Swift\",\n",
    "        \"Genre\": \"Pop\",\n",
    "        \"Song\": \"Now That We Dont Talk (Taylor's Version)(From The Vault)\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Zach Bryan\",\n",
    "        \"Genre\": \"Country\",\n",
    "        \"Song\": \"Something in the Orange\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Gunna\",\n",
    "        \"Genre\": \"Rap\",\n",
    "        \"Song\": \"fukumean\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Bobby Helms\",\n",
    "        \"Genre\": \"Holiday\",\n",
    "        \"Song\": \"Jingle Bell Rock\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Tate McRae\",\n",
    "        \"Genre\": \"Pop\",\n",
    "        \"Song\": \"exes\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Morgan Wallen\",\n",
    "        \"Genre\": \"Country\",\n",
    "        \"Song\": \"Last Night\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Bad Bunny\",\n",
    "        \"Genre\": \"Latin Urbano\",\n",
    "        \"Song\": \"MONACO\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"SZA\",\n",
    "        \"Genre\": \"R&B\",\n",
    "        \"Song\": \"Snooze\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Wham!\",\n",
    "        \"Genre\": \"Holiday\",\n",
    "        \"Song\": \"Last Christmas - Single Version\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Fuerza Regida, Marshmello\",\n",
    "        \"Genre\": \"Latin Urbano\",\n",
    "        \"Song\": \"HARLEY QUINN\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Olivia Rodrigo\",\n",
    "        \"Genre\": \"Pop\",\n",
    "        \"Song\": \"Can't Catch Me Now - from The Hunger Games: The Ballard of Songbirds & Snakes\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Drake, Sexyy Red, SZA\",\n",
    "        \"Genre\": \"Rap\",\n",
    "        \"Song\": \"Rich Baby Daddy (feat. Sexyy Red & SZA)\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Andy Williams\",\n",
    "        \"Genre\": \"Holiday\",\n",
    "        \"Song\": \"It's the Most Wonderful Time of the Year\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"SZA\",\n",
    "        \"Genre\": \"R&B\",\n",
    "        \"Song\": \"Kill Bill\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Olivia Rodrigo\",\n",
    "        \"Genre\": \"Pop\",\n",
    "        \"Song\": \"Vampire\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Taylor Swift\",\n",
    "        \"Genre\": \"Pop\",\n",
    "        \"Song\": \"Slut! (Taylor's Version)(From The Vault)\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Frank Ocean\",\n",
    "        \"Genre\": \"R&B\",\n",
    "        \"Song\": \"Pink + White\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Drake\",\n",
    "        \"Genre\": \"Rap\",\n",
    "        \"Song\": \"Virginia Beach\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Tyler, The Creator, Kali Uchis\",\n",
    "        \"Genre\": \"Indie\",\n",
    "        \"Song\": \"See You Again (feat. Kali Uchis)\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Kenya Grace\",\n",
    "        \"Genre\": \"Dance\",\n",
    "        \"Song\": \"Strangers\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"J. Cole\",\n",
    "        \"Genre\": \"Hip-Hop\",\n",
    "        \"Song\": \"No Role Modelz\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Dean Martin\",\n",
    "        \"Genre\": \"Holiday\",\n",
    "        \"Song\": \"Let It Snow! Let It Snow! Let It Snow!\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Drake, J. Cole\",\n",
    "        \"Genre\": \"Rap\",\n",
    "        \"Song\": \"Evil Ways (feat. J. Cole)\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Michael Buble\",\n",
    "        \"Genre\": \"Holiday\",\n",
    "        \"Song\": \"It's Beginning to Look a lot like Christmas\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"The Neighbourhood\",\n",
    "        \"Genre\": \"Indie\",\n",
    "        \"Song\": \"Sweater Weather\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Burl Ives\",\n",
    "        \"Genre\": \"Holiday\",\n",
    "        \"Song\": \"A Holly Jolly Christmas\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Morgan Wallen\",\n",
    "        \"Genre\": \"Country\",\n",
    "        \"Song\": \"You Proof\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Calle 24, Chino Pacas, Fuerza Regida\",\n",
    "        \"Genre\": \"Latin Urbano\",\n",
    "        \"Song\": \"Que Onda\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Peso Plum, Gabito Ballesteros, Junior H\",\n",
    "        \"Genre\": \"Latin Urbano\",\n",
    "        \"Song\": \"LADY GAGA\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Morgan Wallen\",\n",
    "        \"Genre\": \"Country\",\n",
    "        \"Song\": \"Thinkin' Bout Me\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Morgan Wallen\",\n",
    "        \"Genre\": \"Country\",\n",
    "        \"Song\": \"Wasted On You\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Dua Lipa\",\n",
    "        \"Genre\": \"Pop\",\n",
    "        \"Song\": \"Houdini\"\n",
    "    },\n",
    "    {\n",
    "        \"Artist name\": \"Tyla\",\n",
    "        \"Genre\": \"Pop\",\n",
    "        \"Song\": \"water\"\n",
    "    }\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cac61c",
   "metadata": {},
   "source": [
    "#### prototype prompt\n",
    "* use TDD to rapidly iterate and refine prompt\n",
    "* lets set up the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4c0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to change this, just run this cell\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "import openai\n",
    "\n",
    "# Define a function to get the AI's reply using the OpenAI API\n",
    "def get_ai_reply(message, model=\"gpt-3.5-turbo\", system_message=None, temperature=0, message_history=[]):\n",
    "    # Initialize the messages list\n",
    "    messages = []\n",
    "    \n",
    "    # Add the system message to the messages list\n",
    "    if system_message is not None:\n",
    "        messages += [{\"role\": \"system\", \"content\": system_message}]\n",
    "\n",
    "    # Add the message history to the messages list\n",
    "    if message_history is not None:\n",
    "        messages += message_history\n",
    "    \n",
    "    # Add the user's message to the messages list\n",
    "    messages += [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # Make an API call to the OpenAI ChatCompletion endpoint with the model and messages\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    # Extract and return the AI's response from the API response\n",
    "    return completion.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4db745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b50ba7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Give me a song recommendation!\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:331\u001b[0m, in \u001b[0;36mAPIRequestor.handle_error_response\u001b[1;34m(self, rbody, rcode, resp, rheaders, stream_error)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     error_data \u001b[38;5;241m=\u001b[39m \u001b[43mresp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43merror\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGive me a song recommendation!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m expected_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSure! Can you please provide me with a song that you like or a genre that you prefer?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mtest_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m, in \u001b[0;36mtest_helper\u001b[1;34m(prompt, input, expected_value, message_history)\u001b[0m\n\u001b[0;32m      8\u001b[0m         prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mget_ai_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsserting that output \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is equal to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m, in \u001b[0;36mget_ai_reply\u001b[1;34m(message, model, system_message, temperature, message_history)\u001b[0m\n\u001b[0;32m     20\u001b[0m messages \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: message}]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Make an API call to the OpenAI ChatCompletion endpoint with the model and messages\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Extract and return the AI's response from the API response\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    216\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:620\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    613\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    614\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    615\u001b[0m         )\n\u001b[0;32m    616\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    617\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 620\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    626\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    627\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:683\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    681\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_error_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_error\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:333\u001b[0m, in \u001b[0;36mAPIRequestor.handle_error_response\u001b[1;34m(self, rbody, rcode, resp, rheaders, stream_error)\u001b[0m\n\u001b[0;32m    331\u001b[0m     error_data \u001b[38;5;241m=\u001b[39m resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mAPIError(\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid response object from API: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m (HTTP response code \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (rbody, rcode),\n\u001b[0;32m    336\u001b[0m         rbody,\n\u001b[0;32m    337\u001b[0m         rcode,\n\u001b[0;32m    338\u001b[0m         resp,\n\u001b[0;32m    339\u001b[0m     )\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal_message\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_data:\n\u001b[0;32m    342\u001b[0m     error_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m error_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal_message\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mAPIError\u001b[0m: Invalid response object from API: 'Internal Server Error' (HTTP response code was 500)"
     ]
    }
   ],
   "source": [
    "def test_helper(prompt, input, expected_value=\"\", message_history=[]):\n",
    "    for message in history:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "        if role == \"user\":\n",
    "            prefix = \"User: \"\n",
    "        else:\n",
    "            prefix = \"AI: \"\n",
    "    print(f\"Input: {input}\")\n",
    "    output = get_ai_reply(input, system_message=prompt, message_history=history)\n",
    "    print(f\"Output: {output}\")\n",
    "    print(f\"Asserting that output '{output}' is equal to '{expected_value}' \")\n",
    "    assert output == expected_value\n",
    "        \n",
    "# this is a multi-line string\n",
    "prompt=\"\"\"\"\n",
    "You are a personalized music recommender.\n",
    "\n",
    "User: Recommend a song similar to \"{input_song}\" from the top 100 trending songs.\n",
    "Expected AI Response: \"{recommended_song}\" \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#### Testing Typical Input\n",
    "\n",
    "\"\"\"\n",
    "User: Give me a song recommendation!\n",
    "Expected AI Response: Sure! Can you please provide me with a song that you like or a genre that you prefer? \n",
    "\"\"\"\n",
    "history = []\n",
    "input = \"Give me a song recommendation!\"\n",
    "expected_value = \"Sure! Can you please provide me with a song that you like or a genre that you prefer?\"\n",
    "test_helper(prompt, input, expected_value=expected_value, message_history=history)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4b5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the UI using gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b0bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "import gradio as gr\n",
    "import openai\n",
    "\n",
    "# Define a function to get the AI's reply using the OpenAI API\n",
    "def get_ai_reply(message, model=\"gpt-3.5-turbo\", system_message=None, temperature=0, message_history=[]):\n",
    "    # Initialize the messages list\n",
    "    messages = []\n",
    "    \n",
    "    # Add the system message to the messages list\n",
    "    if system_message is not None:\n",
    "        messages += [{\"role\": \"system\", \"content\": system_message}]\n",
    "\n",
    "    # Add the message history to the messages list\n",
    "    if message_history is not None:\n",
    "        messages += message_history\n",
    "    \n",
    "    # Add the user's message to the messages list\n",
    "    messages += [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # Make an API call to the OpenAI ChatCompletion endpoint with the model and messages\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # Extract and return the AI's response from the API response\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "# Define a function to handle the chat interaction with the AI model\n",
    "def chat(message, chatbot_messages, history_state):\n",
    "    # Initialize chatbot_messages and history_state if they are not provided\n",
    "    chatbot_messages = chatbot_messages or []\n",
    "    history_state = history_state or []\n",
    "    \n",
    "    # Try to get the AI's reply using the get_ai_reply function\n",
    "    try:\n",
    "        prompt = \"\"\"\n",
    "        You are a personalized music recommender.\n",
    "\n",
    "        User: Recommend a song similar to \"{input_song}\" from the top 100 trending songs.\n",
    "        Expected AI Response: \"{recommended_song}\" \n",
    "\n",
    "        \"\"\"\n",
    "        ai_reply = get_ai_reply(message, model=\"gpt-3.5-turbo\", system_message=prompt.strip(), message_history=history_state)\n",
    "            \n",
    "        # Append the user's message and the AI's reply to the chatbot_messages list\n",
    "        chatbot_messages.append((message, ai_reply))\n",
    "\n",
    "        # Append the user's message and the AI's reply to the history_state list\n",
    "        history_state.append({\"role\": \"user\", \"content\": message})\n",
    "        history_state.append({\"role\": \"assistant\", \"content\": ai_reply})\n",
    "\n",
    "        # Return None (empty out the user's message textbox), the updated chatbot_messages, and the updated history_state\n",
    "    except Exception as e:\n",
    "        # If an error occurs, raise a Gradio error\n",
    "        raise gr.Error(e)\n",
    "        \n",
    "    return None, chatbot_messages, history_state\n",
    "\n",
    "# Define a function to launch the chatbot interface using Gradio\n",
    "def get_chatbot_app():\n",
    "    # Create the Gradio interface using the Blocks layout\n",
    "    with gr.Blocks() as app:\n",
    "        # Create a chatbot interface for the conversation\n",
    "        chatbot = gr.Chatbot(label=\"Conversation\")\n",
    "        # Create a textbox for the user's message\n",
    "        message = gr.Textbox(label=\"Message\")\n",
    "        # Create a state object to store the conversation history\n",
    "        history_state = gr.State()\n",
    "        # Create a button to send the user's message\n",
    "        btn = gr.Button(value=\"Send\")\n",
    "\n",
    "        # Connect the send button to the chat function\n",
    "        btn.click(chat, inputs=[message, chatbot, history_state], outputs=[message, chatbot, history_state])\n",
    "        # Return the app\n",
    "        return app\n",
    "        \n",
    "# Call the launch_chatbot function to start the chatbot interface using Gradio\n",
    "app = get_chatbot_app()\n",
    "app.queue()  # this is to be able to queue multiple requests at once\n",
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c90d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4fe4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
